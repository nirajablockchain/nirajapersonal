{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1eWAmesrW99p7e1nah5bipn0zikMb8XYC","timestamp":1681681065978},{"file_id":"1M-TJHMpl2kwXpDH27lx8k33qQ2MB2P7M","timestamp":1678849997115}],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["# Talk to Alpaca-LoRA\n","\n","This notebook contains minimal code for running [Alpaca-LoRA](https://github.com/tloen/alpaca-lora/) for demonstration purposes. Please check the repo for more details."],"metadata":{"id":"6ExAe1UCWn5E"}},{"cell_type":"code","source":["!pip install bitsandbytes\n","!pip install -q datasets loralib sentencepiece\n","!pip install -q git+https://github.com/zphang/transformers@c3dc391\n","!pip install -q git+https://github.com/huggingface/peft.git\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_pz8MuY84Qh","executionInfo":{"status":"ok","timestamp":1681697378641,"user_tz":240,"elapsed":66728,"user":{"displayName":"Niraja Blockchain","userId":"00989711568223861163"}},"outputId":"c0f989bc-7c3e-4cec-9e6b-9d4407c2d898"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.38.1-py3-none-any.whl (104.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.38.1\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33m  WARNING: Did not find branch or tag 'c3dc391', assuming revision or ref.\u001b[0m\u001b[33m\n","\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["from peft import PeftModel\n","from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig\n","import torch\n","\n","tokenizer = LLaMATokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n","model = LLaMAForCausalLM.from_pretrained(\n","    \"decapoda-research/llama-7b-hf\",\n","    load_in_8bit=True,\n","    device_map=\"auto\"\n",")\n","model = PeftModel.from_pretrained(model, \"tloen/alpaca-lora-7b\" )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":347},"id":"VucO3HSMoJkz","executionInfo":{"status":"error","timestamp":1681683839337,"user_tz":240,"elapsed":2059,"user":{"displayName":"Niraja Blockchain","userId":"00989711568223861163"}},"outputId":"11eb1d16-283e-4253-cac3-54b9d51e7c63"},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92m<cell line: 6>\u001b[0m:\u001b[94m6\u001b[0m                                                                              \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.9/dist-packages/transformers/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m2578\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2575 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mkey: device_map[key] \u001b[94mfor\u001b[0m key \u001b[95min\u001b[0m device_map.keys() \u001b[94mif\u001b[0m key \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m modu  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2576 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m}                                                                         \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2577 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mcpu\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m device_map_without_lm_head.values() \u001b[95mor\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mdisk\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m device_map_  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2578 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2579 \u001b[0m\u001b[2;90m│   │   │   │   │   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                               \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2580 \u001b[0m\u001b[2;33m│   │   │   │   │   │   \u001b[0m\u001b[33mSome modules are dispatched on the CPU or the disk. Make sure yo\u001b[0m  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m2581 \u001b[0m\u001b[2;33m│   │   │   │   │   │   \u001b[0m\u001b[33mthe quantized model. If you have set a value for `max_memory` yo\u001b[0m  \u001b[31m│\u001b[0m\n","\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mValueError: \u001b[0m\n","                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to \n","fit\n","                        the quantized model. If you have set a value for `max_memory` you should increase that. To \n","have\n","                        an idea of the modules that are set on the CPU or RAM you can print model.hf_device_map.\n","                        \n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 6&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">6</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.9/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2578</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2575 │   │   │   │   │   </span>key: device_map[key] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> key <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> device_map.keys() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> key <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> modu  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2576 │   │   │   │   </span>}                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2577 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"cpu\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> device_map_without_lm_head.values() <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"disk\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> device_map_  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2578 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2579 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2580 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">Some modules are dispatched on the CPU or the disk. Make sure yo</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2581 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">the quantized model. If you have set a value for `max_memory` yo</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>\n","                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to \n","fit\n","                        the quantized model. If you have set a value for `max_memory` you should increase that. To \n","have\n","                        an idea of the modules that are set on the CPU or RAM you can print model.hf_device_map.\n","                        \n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["def generate_prompt(instruction, input=None):\n","    if input:\n","        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{instruction}\n","\n","### Input:\n","{input}\n","\n","### Response:\"\"\"\n","    else:\n","        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{instruction}\n","\n","### Response:\"\"\""],"metadata":{"id":"w3_lzwcqermJ","executionInfo":{"status":"ok","timestamp":1681683843237,"user_tz":240,"elapsed":332,"user":{"displayName":"Niraja Blockchain","userId":"00989711568223861163"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["generation_config = GenerationConfig(\n","    temperature=0.1,\n","    top_p=0.75,\n","    num_beams=4,\n",")\n","\n","def evaluate(instruction, input=None):\n","    prompt = generate_prompt(instruction, input)\n","    inputs = tokenizer(prompt, return_tensors=\"pt\")\n","    input_ids = inputs[\"input_ids\"].cuda()\n","    generation_output = model.generate(\n","        input_ids=input_ids,\n","        generation_config=generation_config,\n","        return_dict_in_generate=True,\n","        output_scores=True,\n","        max_new_tokens=256\n","    )\n","    for s in generation_output.sequences:\n","        output = tokenizer.decode(s)\n","        print(\"Response:\", output.split(\"### Response:\")[1].strip())"],"metadata":{"id":"Egh3beCVRpW5","executionInfo":{"status":"ok","timestamp":1681683848500,"user_tz":240,"elapsed":3,"user":{"displayName":"Niraja Blockchain","userId":"00989711568223861163"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["evaluate(input(\"Instruction: \"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169},"id":"upOB2AQJSW9-","executionInfo":{"status":"error","timestamp":1681697304486,"user_tz":240,"elapsed":8,"user":{"displayName":"Niraja Blockchain","userId":"00989711568223861163"}},"outputId":"0bee1b26-7a11-41b4-a090-76a240e39a62"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-2a48022c2718>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Instruction: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"]}]}]}